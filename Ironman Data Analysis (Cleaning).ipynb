{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae678947",
   "metadata": {},
   "source": [
    "# Triathlon Dataset: Cleaning\n",
    "Several tasks are completed to clean the data before any analysis can be performed.\n",
    " - Merging tables, removing columns and renaming headers\n",
    " - Locations and nationality abbreviations changed to full names\n",
    " - Athlete gender and age groups inconsistencies\n",
    " - Converting time data to integer number of seconds\n",
    " - Testing for times outside of world records or cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c50514b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056cebc",
   "metadata": {},
   "source": [
    "## Importing and merging the data\n",
    "The two csv files, corresponding to the events and the athlete results, are read in. As long as these files are in the same folder as the Python file, the data will be read in correctly. The two files are merged to create a single table following the reading of the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0da57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() #Finds the current working directory. Allows data to be read without changing the filepath.\n",
    "df_events = pd.read_csv(cwd+\"\\postgres_public_tristat_events.csv\")\n",
    "df_stat = pd.read_csv(cwd+\"\\postgres_public_tristat_stat.csv\")\n",
    "\n",
    "#Renaming the columns before merging the dataframes.\n",
    "df_stat.columns = ['event_link', 'gender', 'person_link', 'person_flag', 'person_name',\n",
    "       'age_group', 'swim_time', 't1_time', 'cycle_time', 't2_time', 'run_time', 'finish_time']\n",
    "df_events.columns = ['date', 'location', 'event', 'f_count', 'm_count', 'event_link']\n",
    "\n",
    "#An inner join to merge the two tables. \n",
    "df_full = df_events.merge(df_stat, how='inner', on='event_link')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b99f8",
   "metadata": {},
   "source": [
    "## New columns and removed records\n",
    "Continuing to alter the data to allow for the cleaning and analysis to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6858b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New columns added to help to clean, sort and analyse the data. These are the event links split into individual columns.\n",
    "event_distances, organisers, years, links = [[], [], [], []]\n",
    "\n",
    "for i in df_full['event_link']: #Iterating through the event_link series\n",
    "    years.append(i.split('/')[-1]) #Year of the race\n",
    "    event_distances.append(i.split('/')[-2]) #Selects the race distance and appends to list for every event\n",
    "    organisers.append(i.split('/')[-4]) #Organiser info\n",
    "\n",
    "df_full['year'] = years\n",
    "df_full['year'] = pd.to_datetime(df_full['year']).dt.year\n",
    "df_full['distance'] = event_distances #New column identifies race distance\n",
    "df_full['organiser'] = organisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a151775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records removed: 643861\n"
     ]
    }
   ],
   "source": [
    "#Only wish to have Ironman events of full and half distance.\n",
    "#Remove all other records.\n",
    "df_iron = df_full[(df_full.organiser=='ironman')&((df_full.distance=='half')|(df_full.distance=='full'))].copy()\n",
    "\n",
    "#Removes several columns: date, f_count, m_count, event_link, person_name, organiser.\n",
    "df_iron = df_iron[['event', 'location', 'year', 'distance', 'person_link', 'gender', 'person_flag', 'age_group', 'swim_time', 't1_time', 'cycle_time', 't2_time', 'run_time', 'finish_time']].copy()\n",
    "\n",
    "records_removed = df_full.shape[0] - df_iron.shape[0]\n",
    "print(f'Number of records removed: {records_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a930a",
   "metadata": {},
   "source": [
    "## Unique athlete identifier\n",
    "Cleaning inconsistencies in the athlete information would be easiest if there is an identifier for each athlete. The person_link column appears to provide this. This would allow me to homogenise columns such as gender and nationality for each athlete. However, the person_link is not unique for each athlete. There are several such cases which appear to confirm this.\n",
    " - Athletes with the same name and nationality have the same person_link.\n",
    " - Some of the most common person_link values appear to be placeholders for unknown names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2cdd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>event</th>\n",
       "      <th>f_count</th>\n",
       "      <th>m_count</th>\n",
       "      <th>event_link</th>\n",
       "      <th>gender</th>\n",
       "      <th>person_link</th>\n",
       "      <th>person_flag</th>\n",
       "      <th>person_name</th>\n",
       "      <th>age_group</th>\n",
       "      <th>swim_time</th>\n",
       "      <th>t1_time</th>\n",
       "      <th>cycle_time</th>\n",
       "      <th>t2_time</th>\n",
       "      <th>run_time</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>year</th>\n",
       "      <th>distance</th>\n",
       "      <th>organiser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>609282</th>\n",
       "      <td>2019-07-14</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Ironman UK 2019</td>\n",
       "      <td>173</td>\n",
       "      <td>1166</td>\n",
       "      <td>/rus/result/ironman/uk/full/2019</td>\n",
       "      <td>M</td>\n",
       "      <td>/gbr/profile/cooke-richard</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Cooke, Richard</td>\n",
       "      <td>M40-44</td>\n",
       "      <td>1:13:00</td>\n",
       "      <td>9:30</td>\n",
       "      <td>7:49:53</td>\n",
       "      <td>10:09</td>\n",
       "      <td>4:42:34</td>\n",
       "      <td>14:05:06</td>\n",
       "      <td>2019</td>\n",
       "      <td>full</td>\n",
       "      <td>ironman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719878</th>\n",
       "      <td>2015-06-14</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Ironman Staffordshire 70.3 2015</td>\n",
       "      <td>384</td>\n",
       "      <td>1528</td>\n",
       "      <td>/rus/result/ironman/staffordshire/half/2015</td>\n",
       "      <td>M</td>\n",
       "      <td>/gbr/profile/cooke-richard</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Cooke, Richard</td>\n",
       "      <td>M35-39</td>\n",
       "      <td>35:18</td>\n",
       "      <td>6:58</td>\n",
       "      <td>3:10:22</td>\n",
       "      <td>4:10</td>\n",
       "      <td>1:50:31</td>\n",
       "      <td>5:47:19</td>\n",
       "      <td>2015</td>\n",
       "      <td>half</td>\n",
       "      <td>ironman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date location                            event  f_count  \\\n",
       "609282   2019-07-14      GBR                  Ironman UK 2019      173   \n",
       "1719878  2015-06-14      GBR  Ironman Staffordshire 70.3 2015      384   \n",
       "\n",
       "         m_count                                   event_link gender  \\\n",
       "609282      1166             /rus/result/ironman/uk/full/2019      M   \n",
       "1719878     1528  /rus/result/ironman/staffordshire/half/2015      M   \n",
       "\n",
       "                        person_link person_flag     person_name age_group  \\\n",
       "609282   /gbr/profile/cooke-richard         GBR  Cooke, Richard    M40-44   \n",
       "1719878  /gbr/profile/cooke-richard         GBR  Cooke, Richard    M35-39   \n",
       "\n",
       "        swim_time t1_time cycle_time t2_time run_time finish_time  year  \\\n",
       "609282    1:13:00    9:30    7:49:53   10:09  4:42:34    14:05:06  2019   \n",
       "1719878     35:18    6:58    3:10:22    4:10  1:50:31     5:47:19  2015   \n",
       "\n",
       "        distance organiser  \n",
       "609282      full   ironman  \n",
       "1719878     half   ironman  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This person_link corresponds to two different athletes.\n",
    "#My father has completed Ironman UK 2019, but not the other event.\n",
    "df_full[df_full['person_link']=='/gbr/profile/cooke-richard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2500ad1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/tpe/profile/triathlete-taiwan-2    255\n",
       "/jpn/profile/ueda-ai                207\n",
       "/ukr/profile/elistratova-yuliya     175\n",
       "/jpn/profile/hosoda-yuichi          171\n",
       "/ukr/profile/sapunov-danylo         168\n",
       "Name: person_link, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The most common person_link values. 'triathlete-taiwan-2' probably isn't an individual person.\n",
    "df_full['person_link'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc035ed",
   "metadata": {},
   "source": [
    "## Replacing abbreviated locations/nationalities\n",
    "\n",
    "During my analysis, it was revealed that the abbreviated country names are unclear for all except the most common nations. I would like to replace the abbreviations with full names for the countries. I have used the country_converter package to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e076e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the package and setting up a method to allow for the codes to be converted to names.\n",
    "import country_converter as coco\n",
    "cc = coco.CountryConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f10e98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries = df_iron.copy()\n",
    "\n",
    "nationalities = list(df_countries['person_flag'].unique()) #List of unique country codes.\n",
    "nationalities = [x for x in nationalities if str(x) != 'nan'] #Remove the null value in the list. Causes an issue during the use of country_converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bfcb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAF not found in IOC\n",
      "MTQ not found in IOC\n",
      "NCL not found in IOC\n",
      "PYF not found in IOC\n",
      "IMN not found in IOC\n",
      "CCK not found in IOC\n",
      "CCK not found in Eora\n",
      "CUW not found in IOC\n",
      "CUW not found in Eora\n",
      "GGY not found in IOC\n",
      "JEY not found in IOC\n",
      "NIU not found in IOC\n",
      "REU not found in IOC\n",
      "GLP not found in IOC\n",
      "GUF not found in IOC\n",
      "HMD not found in IOC\n",
      "HMD not found in Eora\n",
      "GIB not found in IOC\n",
      "FRO not found in IOC\n",
      "SPM not found in IOC\n",
      "BLM not found in IOC\n",
      "MAC not found in IOC\n",
      "XKX not found in IOC\n",
      "XKX not found in Eora\n",
      "ATA not found in IOC\n",
      "ATA not found in Eora\n",
      "ALA not found in IOC\n",
      "GRL not found in IOC\n",
      "FLK not found in IOC\n",
      "SJM not found in IOC\n",
      "IOT not found in IOC\n",
      "IOT not found in Eora\n",
      "CXR not found in IOC\n",
      "CXR not found in Eora\n",
      "MSR not found in IOC\n",
      "PCN not found in IOC\n",
      "SGS not found in IOC\n",
      "SGS not found in Eora\n",
      "BVT not found in IOC\n",
      "BVT not found in Eora\n",
      "UMI not found in IOC\n",
      "UMI not found in Eora\n",
      "NFK not found in IOC\n",
      "SXM not found in IOC\n",
      "SXM not found in Eora\n",
      "MYT not found in IOC\n",
      "MNP not found in IOC\n",
      "SHN not found in IOC\n",
      "ATF not found in IOC\n",
      "ATF not found in Eora\n",
      "WLF not found in IOC\n",
      "BES not found in IOC\n",
      "BES not found in Eora\n",
      "TCA not found in IOC\n",
      "SCG not found in IOC\n",
      "SCG not found in Eora\n",
      "VAT not found in IOC\n",
      "AIA not found in IOC\n",
      "ESH not found in IOC\n",
      "TKL not found in IOC\n",
      "AHO not found in IOC\n",
      "AHO not found in Eora\n"
     ]
    }
   ],
   "source": [
    "country_conversion_dict = {} #Dictionary to be populated, then used to clean my data.\n",
    "\n",
    "#Iterate through the codes, converting one at a time\n",
    "for i in nationalities:\n",
    "    short_name = cc.convert(names=[i], src='IOC', to='name_short', not_found=None) #Converts based on the IOC codes (Olympic countries only)\n",
    "    if short_name == i:\n",
    "        short_name = cc.convert(names=[i], src='Eora', to='name_short', not_found=None) #Converts based on Eora - a global supply chain database\n",
    "    \n",
    "    country_conversion_dict[i]=short_name #Key-value pairs for conversion added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40614452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some codes filled manually. Tends to be groups of islands or Antarctic territories belonging to European nations/USA.\n",
    "country_conversion_dict.update({\n",
    "    'CCK':'Cocos (Keeling) Islands', 'CUW':'Curacao', 'HMD':'Heard Island and McDonald Islands', 'XKX':'Kosovo', 'ATA':'Antarctica',\n",
    "    'IOT':'British Indian Ocean Territory', 'CXR':'Christmas Island', 'SGS':'South Georgia and the South Sandwich Islands', 'BVT':'Bouvet Island', 'UMI':'United States Minor Outlying Islands',\n",
    "    'SXM':'Sint Maarten', 'ATF':'French Southern and Antarctic Territories', 'BES':'Bonaire, Saint Eustatius and Saba', 'SCG':'Serbia and Montenegro', 'AHO':'Netherlands Antilles'\n",
    "})\n",
    "\n",
    "#This converts the abbreviations to the full names for both columns.\n",
    "df_countries.replace({'person_flag': country_conversion_dict, 'location': country_conversion_dict}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33782aad",
   "metadata": {},
   "source": [
    "## Removing events with missing nationalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36289b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts number of records with null person_flag values per event.\n",
    "df_missing_nationalities_by_event = df_countries[df_countries.person_flag.isnull()][['event', 'location']].groupby(by='event').count()\n",
    "\n",
    "#Calculate proportion of missing values to decide whether to remove events from the dataset.\n",
    "competitors_by_event = df_countries.groupby('event').count()['location'] #Counts number of participants per event\n",
    "df_missing_nationalities_by_event = df_missing_nationalities_by_event.merge(competitors_by_event, how='inner', on='event')\n",
    "df_missing_nationalities_by_event.columns = ['number_missing', 'total_competitors']\n",
    "\n",
    "#Proportions will be used to highlight events that haven't recorded the nationalities.\n",
    "df_missing_nationalities_by_event['proportion'] = df_missing_nationalities_by_event['number_missing']/df_missing_nationalities_by_event['total_competitors']\n",
    "\n",
    "events_to_remove = df_missing_nationalities_by_event[df_missing_nationalities_by_event['proportion']>0.1].index #Seems like 0.1 is a good compromise for keeping most data, removing most missing values.\n",
    "\n",
    "df_clean_countries = df_countries[~df_countries.event.isin(events_to_remove)] #Events are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe8955",
   "metadata": {},
   "source": [
    "## Cleaning the age_group column\n",
    "\n",
    "There are a set of standard Ironman age groups. Additionally, there are age groups designating relay teams and disability categories. There are also several instances where the age group has been mis-recorded. I have decided to retain the competitors with standard age groups. This will simplify the analysis by reducing the number of categories whie also cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1f1b437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M40-44       330356\n",
       "M35-39       311927\n",
       "M45-49       263551\n",
       "M30-34       256918\n",
       "M50-54       174098\n",
       "              ...  \n",
       "M5                1\n",
       "MTBC              1\n",
       "MINMEMORY         1\n",
       "M65+              1\n",
       "M90+              1\n",
       "Name: age_group, Length: 129, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_age = df_clean_countries.copy()\n",
    "df_age['age_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e9d91c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2196844 athletes with standard age grouping; 31603 with non-standard age grouping.\n"
     ]
    }
   ],
   "source": [
    "#The age group boundaries according to the Ironman website.\n",
    "iron_groups = ['F18-24','F25-29','F30-34','F35-39','F40-44','F45-49','F50-54','F55-59','F60-64','F65-69','F70-74','F75-79','F80-84','F85-89','F90+',\n",
    "          'M18-24','M25-29','M30-34','M35-39','M40-44','M45-49','M50-54','M55-59','M60-64','M65-69','M70-74','M75-79','M80-84','M85-89','M90+',\n",
    "          'MPRO','FPRO']\n",
    "\n",
    "#Any other age groups found in the dataset.\n",
    "other_groups = ['MPC','FPC','PC','MST','FST','M','F','MRELAY','FRELAY','MEX','FNKNOWN','M20-24','F20-24',\n",
    "                'M17-18','FJ19','M15-16','MTEAM','FTEAM','M15-19','RELAY','F15-19','MCM18-24','MCM25-29','FCF18-24',\n",
    "                'MU6','M18-25','M80+','FPARA','MCOL','FCOL','MNKNOWN','M18','M25','M30','M35','M45','M40','F25','M50',\n",
    "                'F18','M55','F30','F40','F35','F45','M60','F50','M70','F55','M65','F65','F60','MPRO30-34','MPRO40-44',\n",
    "                'MPRO25-29','MPRO35-39','FPRO30-34','FPRO35-39','M0-44','MAWAD','ML-Z','FRELAYCOED','MRELAYCOED',\n",
    "                'MCLYDESDALES','MAQUABIKE','FATHENAS','MPSL-Z','MINMEMORY','FPROILIPINOPRO','M65+','M19UND','F19UND',\n",
    "                'M0','MNOAGE','MCLY','FCLY','MTBC','M5','M54','F44','M4','M9','F75+','M70+','M70-','MIXED','MHERREN',\n",
    "                'MFIR','FCOUPLES','MCOUPLES','MDA','40-44','U','MX-RLY','MCLY-U','M-RLY','FX-RLY','F-RLY','FATH-U',\n",
    "                'MCLY-O','FATH-O','M75-99','MPROEN','MFPROO','MILITARY','M7-9','F7-9','50-54','MHANDCYCLE','45-49',\n",
    "                '35-39','30-34','55-59','M10-14','60-64','25-29','M70-99','15-19','20-24','70-74','F90-94','65-69',\n",
    "                '75-79','F30-44','M25-30','M55-60','F60-6','F45-50']\n",
    "\n",
    "\n",
    "standard_group = df_age[df_age['age_group'].isin(iron_groups)]['age_group'].size #Number athletes with the standard groups\n",
    "other_group = correct_group = df_age[df_age['age_group'].isin(other_groups)]['age_group'].size #Number with non-standard groups\n",
    "\n",
    "print(f'There are {standard_group} athletes with standard age grouping; {other_group} with non-standard age grouping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e1e30f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ironman Florida 70.3 2022', 0.9993279569892473],\n",
       " ['Ironman New Zealand 2022', 1.0]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating proportions of competitors at each event with the standard age groups.\n",
    "test = df_age.copy()\n",
    "\n",
    "proportions = []\n",
    "\n",
    "for i in test['event'].unique():\n",
    "    event = test[test.event==i] #Dataframe containing the records for a single event\n",
    "    iron = event[event['age_group'].isin(iron_groups)]['age_group'].size #Number records with standard age groups\n",
    "    non_iron = event[event['age_group'].isin(other_groups)]['age_group'].size #Number with non-standard groups\n",
    "    proportions.append([i, iron/(iron+non_iron)]) #Append event name and the proportion to be analysed later\n",
    "    \n",
    "proportions[:2] #Both Florida and NZ have the correct age groups!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65f075e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the events with high proportions of non-standard age groups.\n",
    "events_bad_groups = []\n",
    "\n",
    "for i in range(len(proportions)):\n",
    "    if proportions[i][1] < 0.1: #Increasing to 0.9 only yields an extra 2k entrants with non-standard age groups.\n",
    "        events_bad_groups.append(proportions[i][0])\n",
    "        \n",
    "#Removing all records pertaining to the untrustworthy events.\n",
    "df_events_removed = df_age[~df_age['event'].isin(events_bad_groups)].copy()\n",
    "\n",
    "#Removing the remainder of the records with the non-standard age groups.\n",
    "df_clean_age = df_events_removed[df_events_removed['age_group'].isin(iron_groups)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c4f85",
   "metadata": {},
   "source": [
    "## Cleaning the gender column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e8a5f",
   "metadata": {},
   "source": [
    "### Attempt 1: Standardising gender by person_link\n",
    "This method relied on a unique person_link for each athlete, by converting all values of gender to be the same for each. However, since the person_link is not a unique identifier, this technique is flawed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a7c09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def gender_string_check(s):\n",
    "    '''A function used to check if all of an athlete's recorded genders match.\n",
    "    Allow these athletes to be removed from the gender-correcting analysis.'''\n",
    "    \n",
    "    if s == len(s) * s[0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Passing the dataframe of results into this function returns another dataframe with a cleaned column.\n",
    "#Records are removed if there is no discernible mode gender for each person link.\n",
    "def gender_correct(x):\n",
    "    '''Performs the gender corrections. Where a majority gender is recorded, all values are changed to this gender.\n",
    "    If there is no majority, gender values replaced with a NaN so that records can be removed.\n",
    "    '''\n",
    "    \n",
    "    df_temp = x[['person_link', 'gender']].copy()\n",
    "    df_temp = df_temp.groupby(by='person_link').sum() #Creates string of all gender values combined eg FFFFFMFF\n",
    "    df_temp.drop(df_temp[df_temp.gender.map(gender_string_check) == True].index, inplace=True) #Removes records with consistent gender, eg M or FFF\n",
    "    \n",
    "    gender_update = x.copy()\n",
    "    for i in df_temp.index: #Loop through every athlete with inconsistent gender records\n",
    "        count = Counter(df_temp.loc[i, 'gender']) #Returns dictionary with count of each M/F value eg {'M': 5, 'F': 3}\n",
    "\n",
    "        if count['F'] == count['M']: #Cannot tell which is the likely correct gender\n",
    "            gender_update.loc[gender_update[gender_update['person_link']==i].index, 'gender'] = np.nan\n",
    "        elif count['F'] > count['M']: #Change all to 'F'\n",
    "            gender_update.loc[gender_update[gender_update['person_link']==i].index, 'gender'] = 'F'\n",
    "        else: #Change all to 'M'\n",
    "            gender_update.loc[gender_update[gender_update['person_link']==i].index, 'gender'] = 'M'\n",
    "    \n",
    "    \n",
    "    return gender_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09d89c",
   "metadata": {},
   "source": [
    "### Attempt 2: Comparing gender and age group\n",
    "\n",
    "- Can make sure that the gender specified in the gender column matches the one specified by the age group.\n",
    "- There are 249 records which have non-matching genders. I will just remove them. It is both too many to go through manually and not enough to be concerned about losing too much data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7707701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = df_clean_age.copy() #Taking forward the most clean version of the data.\n",
    "\n",
    "df_gender.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa64675a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_link</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6759</th>\n",
       "      <td>/aus/profile/williamson-stuart</td>\n",
       "      <td>M</td>\n",
       "      <td>F35-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16361</th>\n",
       "      <td>/phi/profile/guz-dela-rosa</td>\n",
       "      <td>F</td>\n",
       "      <td>M35-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17905</th>\n",
       "      <td>/usa/profile/lazarus-andre</td>\n",
       "      <td>F</td>\n",
       "      <td>M18-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27947</th>\n",
       "      <td>/swe/profile/gerremo-hans</td>\n",
       "      <td>F</td>\n",
       "      <td>M70-74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29207</th>\n",
       "      <td>/pan/profile/aued-lysa</td>\n",
       "      <td>M</td>\n",
       "      <td>F40-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191398</th>\n",
       "      <td>/ger/profile/apel-stefanie</td>\n",
       "      <td>M</td>\n",
       "      <td>F25-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191442</th>\n",
       "      <td>/usa/profile/castaneda-angelika</td>\n",
       "      <td>M</td>\n",
       "      <td>F50-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191459</th>\n",
       "      <td>/sui/profile/wieneke-silvia</td>\n",
       "      <td>M</td>\n",
       "      <td>F40-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192637</th>\n",
       "      <td>/ger/profile/hauhann-katrin</td>\n",
       "      <td>M</td>\n",
       "      <td>F25-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192722</th>\n",
       "      <td>/ger/profile/seidel-susanne</td>\n",
       "      <td>M</td>\n",
       "      <td>F25-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             person_link gender age_group\n",
       "6759      /aus/profile/williamson-stuart      M    F35-39\n",
       "16361         /phi/profile/guz-dela-rosa      F    M35-39\n",
       "17905         /usa/profile/lazarus-andre      F    M18-24\n",
       "27947          /swe/profile/gerremo-hans      F    M70-74\n",
       "29207             /pan/profile/aued-lysa      M    F40-44\n",
       "...                                  ...    ...       ...\n",
       "2191398       /ger/profile/apel-stefanie      M    F25-29\n",
       "2191442  /usa/profile/castaneda-angelika      M    F50-54\n",
       "2191459      /sui/profile/wieneke-silvia      M    F40-44\n",
       "2192637      /ger/profile/hauhann-katrin      M    F25-29\n",
       "2192722      /ger/profile/seidel-susanne      M    F25-29\n",
       "\n",
       "[248 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gender_age = df_gender[['gender', 'age_group']]\n",
    "\n",
    "#Aiming to create a new column which has the age group gender extracted.\n",
    "age_genders = []\n",
    "\n",
    "for i in df_gender['age_group']: #Iterating through the event_link series\n",
    "    age_genders.append(str(i)[0]) #Gender from age group\n",
    "    \n",
    "df_gender['age_gender'] = age_genders\n",
    "\n",
    "df_gender[df_gender['gender'] != df_gender['age_gender']][['person_link', 'gender', 'age_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29477946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event                                   \n",
       "Ironman Durban 70.3 2021                    17\n",
       "Ironman UK 2006                              7\n",
       "Ironman UK 2007                              6\n",
       "Ironman Europe 1994                          5\n",
       "Ironman Dubai 70.3 2021                      4\n",
       "                                            ..\n",
       "Ironman Japan 2007                           1\n",
       "Ironman Indian Wells La Quinta 70.3 2022     1\n",
       "Ironman Indian Wells La Quinta 70.3 2019     1\n",
       "Ironman Haugesund Norway 70.3 2018           1\n",
       "Ironman Zell am See-Kaprun 70.3 2022         1\n",
       "Length: 138, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Are there any races particularly guilty of poor administration?\n",
    "df_gender[df_gender['gender'] != df_gender['age_gender']][['event']].value_counts()\n",
    "\n",
    "#Top offenders do not have enough mistakes to worry about the whole race. At worst it is 1/50 records from the Durban race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81323237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_gender = df_gender[df_gender['gender'] == df_gender['age_gender']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4e164",
   "metadata": {},
   "source": [
    "## Converting the time columns to seconds values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "739eda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_function(x):\n",
    "    '''Convert a time string into an integer number of seconds. \n",
    "    Accepts inputs in the M:SS, MM:SS, H:MM:SS and HH:MM:SS formats.'''\n",
    "    \n",
    "    #Adds minutes and hours values so that all times have hours, minutes and seconds components.\n",
    "    if len(x)==5:\n",
    "        x = '0:'+x #Adds an hours components to MM:SS times.\n",
    "    elif len(x)==4:\n",
    "        x = '0:0'+x #Adds an hours and second minutes digit to M:SS times.\n",
    "\n",
    "    date_format = '%H:%M:%S'\n",
    "    time = dt.strptime(x, date_format) #Converts time string to datetime format.\n",
    "    time = time.second + time.minute*60 + time.hour*3600 #Calculates integer number of seconds.\n",
    "    \n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a29b643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_clean_gender.copy()\n",
    "df_time.drop(['age_gender'], axis=1, inplace=True)\n",
    "\n",
    "#Applies the time function to every time-based column in the dataframe.\n",
    "df_time.iloc[:, 8:] = df_time.iloc[:, 8:].applymap(lambda x:time_function(x)) #Assumes that the columns are the rightmost set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561ba56",
   "metadata": {},
   "source": [
    "## Cleaning the time columns\n",
    "There are two potential problems with the time columns\n",
    " - Missing data (cancelled swims, no transition times, etc)\n",
    " - Unrealistic times (quicker than record pace, slower than cutoff times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2602563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "swim_time       62392\n",
       "t1_time        139291\n",
       "cycle_time      17230\n",
       "t2_time         40643\n",
       "run_time         3256\n",
       "finish_time         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Every record has a finish time, but other calues are missing.\n",
    "print('Number of missing values per column:')\n",
    "df_time[['swim_time', 't1_time', 'cycle_time', 't2_time', 'run_time', 'finish_time']].isin([0]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f6aaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the events which have full time information (non-zero time for all 6 columns)\n",
    "test = df_time.copy()\n",
    "\n",
    "event_indicators = []\n",
    "\n",
    "for i in test['event'].unique(): #Iterate through the events\n",
    "    df_event = test[test['event']==i][['event', 'swim_time', 't1_time', 'cycle_time', 't2_time', 'run_time', 'finish_time']]\n",
    "    column_zeros = df_event.isin([0]).sum(axis=0) #Number of zeros per column calculated for each event\n",
    "    \n",
    "    #This loop returns a True value for the indicator if there are no missing times. False is there's a missing value.\n",
    "    for j in column_zeros:\n",
    "        if j==0:\n",
    "            indicator=True\n",
    "        else:\n",
    "            indicator=False\n",
    "            break\n",
    "            \n",
    "    proportion_zeros = column_zeros.sum()/df_event.iloc[:, 1:].size\n",
    "    \n",
    "    #The indicator reveals whether there is any missing data; the proportion reveals how much missing data.\n",
    "    event_indicators.append([i, indicator, proportion_zeros]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b67131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 489 events with full timing information.\n",
      "1089 events have at least one missing time.\n",
      "Of these, 929 have less than 1% missing times.\n"
     ]
    }
   ],
   "source": [
    "#Some summary statistics on the metrics calculated in the last cell\n",
    "events_missing_times, events_all_times, events_nearly_complete = [[], [], []]\n",
    "\n",
    "for i in event_indicators:\n",
    "    if i[1] == True:\n",
    "        events_all_times.append(i[0]) #Events with no missing times\n",
    "    else:\n",
    "        events_missing_times.append(i[0]) #Events with any amount of information.\n",
    "    \n",
    "    #0.01 is chosen here because it provides good value for number of events, without having too much missing data.\n",
    "    #0.001 has 556 events - lots less events.\n",
    "    #0.1 has 990 events - a very small increase with a large missing data penalty.\n",
    "    if i[2]!=0 and i[2]<0.01: \n",
    "        events_nearly_complete.append(i[0]) #Events with less than 1% missing information.\n",
    "\n",
    "print(f'There are {len(events_all_times)} events with full timing information.')\n",
    "print(f'{len(events_missing_times)} events have at least one missing time.')\n",
    "print(f'Of these, {len(events_nearly_complete)} have less than 1% missing times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "723b14a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630744 records from complete events. 1373055 records from nearly complete events.\n"
     ]
    }
   ],
   "source": [
    "#Dataframe containing the events with complete time information.\n",
    "df_complete = df_time[df_time['event'].isin(events_all_times)].copy()\n",
    "\n",
    "#Dataframe containing the events with less than 1% of missing time information.\n",
    "df_nearly_complete = df_time[df_time['event'].isin(events_nearly_complete)].copy()\n",
    "\n",
    "#Can have triple the amount of data when including the nearly-complete events.\n",
    "print(f'{df_complete.shape[0]} records from complete events. {df_nearly_complete.shape[0]} records from nearly complete events.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b5d5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all records with the zeros from df_nearly_complete (around 4500 records).\n",
    "df_nearly_complete = df_nearly_complete[(df_nearly_complete['swim_time'] != 0)&(df_nearly_complete['t1_time'] != 0)&(df_nearly_complete['cycle_time'] != 0)&(df_nearly_complete.t2_time != 0)&(df_nearly_complete['run_time'] != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3b5fd",
   "metadata": {},
   "source": [
    "## Divergence of the data\n",
    "I will now take forward two dataframes:\n",
    "- All records, regardless of completeness of time column data.\n",
    "- A dataframe with no missing time data. Some events and records will be removed to achieve this.\n",
    "\n",
    "These two dataframes will be used for different purposes when analysing and visualising the data. This will depend only on whether the completeness of time data is required for the analysis in question. For example, calculating an average swim time for full distance Ironman events would be skewed by the presence of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce553aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = df_time.copy()\n",
    "\n",
    "df_complete_time_data = pd.concat([df_complete, df_nearly_complete], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e3405",
   "metadata": {},
   "source": [
    "## Incorrect time data\n",
    "I will set upper and lower bounds on each of the timing splits. This will identify times which are quicker than the world records or slower than the cutoff times. Any records flagged in this way be removed, and events with larger proportions of 'incorrect' times can be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8edb13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_test(x):\n",
    "    '''A function which performs checks on the time columns of the triathlon data.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    x -- triathlon results dataframe.\n",
    "    \n",
    "    Returns:\n",
    "    df_indicators -- a dataframe containing True/False values, indicating whether splits are within the accepted range.\n",
    "    '''\n",
    "    x.reset_index(inplace=True, drop=True) #Ensures that iterating through the rows is successful\n",
    "        \n",
    "    split_indicators = []\n",
    "    \n",
    "    #Iterate through the rows to check the time for its position between the boundaries.\n",
    "    for i in x.index:\n",
    "        #First assign a set of time boundaries based on the race distance of the record\n",
    "        if x.loc[i, 'distance'] == 'full':\n",
    "            swim_low_time , swim_cut_time , bike_low_time , bike_cut_time , run_low_time , finish_low_time , finish_cut_time = [2400, 8400, 14400, 37800, 9000, 27600, 61200]\n",
    "        elif x.loc[i, 'distance'] == 'half':\n",
    "            swim_low_time , swim_cut_time , bike_low_time , bike_cut_time , run_low_time , finish_low_time , finish_cut_time = [1300, 4200, 6900, 19800, 4080, 12360, 30600]\n",
    "\n",
    "        #Setting up some variables which are assigned the True/False values.\n",
    "        swim_low, swim_cut, bike_low, bike_cut, run_low, finish_low, finish_cut = [False, False, False, False, False, False, False]\n",
    "        event = x.loc[i, 'event']\n",
    "\n",
    "        #A series of tests which check whether the splits are within the expected bounds.\n",
    "        #If a time exceeds a cutoff, or is less than a record, the relevant variable is assigned a True value.\n",
    "        if x.loc[i, 'swim_time'] < swim_low_time:\n",
    "            swim_low = True\n",
    "        if x.loc[i, 'swim_time'] > swim_cut_time:\n",
    "            swim_cut = True\n",
    "        if x.loc[i, 'cycle_time'] < bike_low_time:\n",
    "            bike_low = True\n",
    "        if (x.loc[i, 'cycle_time']+x.loc[i, 'swim_time']) > bike_cut_time: \n",
    "            bike_cut = True\n",
    "        if x.loc[i, 'run_time'] < run_low_time:\n",
    "            run_low = True\n",
    "        if x.loc[i, 'finish_time'] < finish_low_time:\n",
    "            finish_low = True\n",
    "        if x.loc[i, 'finish_time'] > finish_cut_time:\n",
    "            finish_cut = True\n",
    "\n",
    "        #Append the index, event name and indicators to a list\n",
    "        split_indicators.append([i, event, swim_low, swim_cut, bike_low, bike_cut, run_low, finish_low, finish_cut])\n",
    "    \n",
    "    #Turning the list of list into a dataframe for further analysis, before returning this dataframe.\n",
    "    time_indicators = pd.DataFrame(split_indicators, columns=['index', 'event', 'swim_low', 'swim_cut', 'bike_low', 'bike_cut', 'run_low', 'finish_low', 'finish_cut'])\n",
    "    time_indicators.set_index('index', inplace=True)\n",
    "    \n",
    "    return time_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be7c92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_indicators = time_test(df_complete_time_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4a11c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times flagged per measure. Mostly faster than expected splits, rather than missed cut offs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "swim_low      30306\n",
       "swim_cut       4636\n",
       "bike_low       8254\n",
       "bike_cut       1438\n",
       "run_low        3555\n",
       "finish_low     3625\n",
       "finish_cut     6075\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events with large numbers of times flagged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event\n",
       "Ironman New Zealand 2012         4795\n",
       "Ironman Muncie 70.3 2012         3831\n",
       "Ironman Des Moines 70.3 2021     2452\n",
       "Ironman Louisville 2018          1942\n",
       "Ironman Boise 70.3 2012          1809\n",
       "Ironman Santa Cruz 70.3 2017     1761\n",
       "Ironman Connecticut 70.3 2019    1708\n",
       "Ironman Chattanooga 70.3 2019    1663\n",
       "Ironman Melbourne 2013           1644\n",
       "Ironman Barcelona 2021           1497\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Some exploratory measures of the results.\n",
    "\n",
    "print('Number of times flagged per measure. Mostly faster than expected splits, rather than missed cut offs.')\n",
    "bad_times_by_boundary = df_time_indicators[['swim_low', 'swim_cut', 'bike_low', 'bike_cut', 'run_low', 'finish_low', 'finish_cut']].sum()\n",
    "display(bad_times_by_boundary)\n",
    "\n",
    "print('Events with large numbers of times flagged.')\n",
    "bad_times_by_event = df_time_indicators.groupby('event').sum().sum(axis=1).sort_values(ascending=False).head(10)\n",
    "display(bad_times_by_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37a0d6",
   "metadata": {},
   "source": [
    "## Cleaning based on the time test\n",
    "\n",
    "A 1% of incorrect times within an event will again be used as a threshold to keep that event in the dataset taken forward for analysis. Once the untrustworthy events are removed, the remaining records with incorrect times are removed individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f53b6be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 274 events with >1% of bad times\n"
     ]
    }
   ],
   "source": [
    "#Looking into the proportions of participants which have suspect times for each event.\n",
    "#Any split which has >1% of flagged times will results in the event being excluded from the timing analysis.\n",
    "\n",
    "#Calculating proportions of incorrect/unlikely times.\n",
    "event_indicator_sums = df_time_indicators.groupby('event').sum() #Number of violations by event by split\n",
    "competitors_by_event = df_complete_time_data.groupby('event').count()['location'] #Counts number of participants per event\n",
    "df_event_indicators = event_indicator_sums.merge(competitors_by_event, how='inner', on='event') #Combines indicators df with the participants series\n",
    "df_event_indicators.columns = ['swim_low', 'swim_cut', 'bike_low', 'bike_cut', 'run_low', 'finish_low', 'finish_cut', 'participants'] #Renames columns\n",
    "df_event_indicators_percentage = df_event_indicators[['swim_low', 'swim_cut', 'bike_low', 'bike_cut', 'run_low', 'finish_low', 'finish_cut']].div(df_event_indicators.participants, axis=0) #Divides summed indicators by number of participants\n",
    "\n",
    "#Now assess which events have high proportions of false timings. \n",
    "events_to_drop = []\n",
    "\n",
    "#Checks for any proportions >=1% in every event, appending the events to a list.\n",
    "for i in df_event_indicators_percentage.index: #Iterate through the events\n",
    "    if df_event_indicators_percentage.loc[i].ge(0.01).any(): #Find events with any row greater than 0.01\n",
    "        events_to_drop.append(i) #Append these events to a list.\n",
    "        \n",
    "print(f'There are {len(events_to_drop)} events with >1% of bad times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1b0cf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event         Ironman New Zealand 2012Ironman New Zealand 20...\n",
       "swim_low                                                    788\n",
       "swim_cut                                                      0\n",
       "bike_low                                                   1391\n",
       "bike_cut                                                      0\n",
       "run_low                                                    1256\n",
       "finish_low                                                 1360\n",
       "finish_cut                                                    0\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "event         Ironman New Zealand 70.3 2012Ironman New Zeala...\n",
       "swim_low                                                      0\n",
       "swim_cut                                                      3\n",
       "bike_low                                                      0\n",
       "bike_cut                                                      0\n",
       "run_low                                                       0\n",
       "finish_low                                                    0\n",
       "finish_cut                                                    6\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Ironman New Zealand 2012 is a half distance event, incorrectly labelled.\n",
    "df_nz_2012 = df_complete_time_data[df_complete_time_data['event']=='Ironman New Zealand 2012']\n",
    "\n",
    "#Indicators with full race distance\n",
    "df_nz_2012_indicators = time_test(df_nz_2012)\n",
    "display(df_nz_2012_indicators.sum())\n",
    "\n",
    "#Change to half distace labels\n",
    "df_nz_2012 = df_nz_2012.assign(distance = 'half', event = 'Ironman New Zealand 70.3 2012')\n",
    "\n",
    "#Indicators with half race distance\n",
    "df_nz_2012_indicators = time_test(df_nz_2012)\n",
    "display(df_nz_2012_indicators.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9275b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 events removed.\n"
     ]
    }
   ],
   "source": [
    "#Remove all of the events with high numbers of bad times.\n",
    "df_time_removed_events = df_complete_time_data[~df_complete_time_data['event'].isin(events_to_drop)].copy()\n",
    "\n",
    "events_before_removal = df_complete_time_data.event.nunique()\n",
    "events_after_removal = df_time_removed_events.event.nunique()\n",
    "\n",
    "print(f'{events_before_removal - events_after_removal} events removed.')\n",
    "\n",
    "#Add Ironman NZ 2012 back in\n",
    "df_time_removed_events = pd.concat([df_time_removed_events, df_nz_2012], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6e2987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times flagged per measure. Mostly faster than expected splits, rather than missed cut offs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "swim_low       384\n",
       "swim_cut      1046\n",
       "bike_low        74\n",
       "bike_cut       437\n",
       "run_low        190\n",
       "finish_low      10\n",
       "finish_cut    1621\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events with large numbers of times flagged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event\n",
       "Ironman Augusta 70.3 2013                 53\n",
       "Ironman Oceanside California 70.3 2012    49\n",
       "Ironman World Championship 2022           37\n",
       "Ironman Copenhagen 2016                   37\n",
       "Ironman Timberman 70.3 2010               35\n",
       "Ironman Texas 70.3 2012                   34\n",
       "Ironman Oceanside California 70.3 2011    34\n",
       "Ironman Eagleman 70.3 2013                34\n",
       "Ironman Mont-Tremblant 70.3 2014          31\n",
       "Ironman Mont-Tremblant 70.3 2013          31\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now re-run the analysis to see what I'm left with.\n",
    "df_new_time_indicators = time_test(df_time_removed_events)\n",
    "\n",
    "print('Number of times flagged per measure. Mostly faster than expected splits, rather than missed cut offs.')\n",
    "display(df_new_time_indicators[['swim_low', 'swim_cut', 'bike_low', 'bike_cut', 'run_low', 'finish_low', 'finish_cut']].sum())\n",
    "\n",
    "print('Events with large numbers of times flagged.')\n",
    "display(df_new_time_indicators.groupby('event').sum().sum(axis=1).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8e26b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 half distance events kept out of 942\n",
      "447 full distance events kept out of 476\n"
     ]
    }
   ],
   "source": [
    "#Appears that a large number of half distance events are affected\n",
    "\n",
    "#Calculation of number of half and full events before any are removed.\n",
    "number_events_before = df_complete_time_data[['event', 'distance']].drop_duplicates()['distance'].value_counts()\n",
    "\n",
    "#Calculation after events have been removed.\n",
    "number_events_after = df_time_removed_events[['event', 'distance']].drop_duplicates()['distance'].value_counts()\n",
    "\n",
    "print(f'{number_events_after[0]} half distance events kept out of {number_events_before[0]}')\n",
    "print(f'{number_events_after[1]} full distance events kept out of {number_events_before[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248acaee",
   "metadata": {},
   "source": [
    "### Removing the remaining records with bad times\n",
    "\n",
    "Achieve the cleanest possible time data for analysis by removing the remaining bad records. This should be the last action done in cleaning the data, unless anything becomes apparent during the visualisation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d38da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting indices of records with a time outside of the accepted ranges.\n",
    "records_to_remove = df_new_time_indicators[df_new_time_indicators[['swim_low', 'swim_cut', 'bike_low', 'bike_cut', 'run_low', 'finish_low', 'finish_cut']].sum(axis=1).ge(1)].index.tolist()\n",
    "\n",
    "df_time_data = df_time_removed_events.drop(records_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01854026",
   "metadata": {},
   "source": [
    "## Exporting the data for analysis\n",
    "\n",
    "The two separate dataframes need to be exported. This will allow for their use in the analysis programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "409f4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data.to_csv(cwd+\"\\data_categorical_analysis.csv\")\n",
    "df_time_data.to_csv(cwd+\"\\data_numerical_analysis.csv\")\n",
    "df_iron.to_csv(cwd+\"\\data_ironman.csv\") #Added an export of the unclean ironman data. I need a dataframe with all of the events present for plotting the event locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
